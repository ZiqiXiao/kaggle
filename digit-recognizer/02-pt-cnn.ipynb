{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Prepare the data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# Create the directory structure and download the data from Kaggle\n",
    "# !mkdir -p ~/DataspellProjects/kaggle/digit-recognizer/data\n",
    "# !cd ~/DataspellProjects/kaggle/digit-recognizer/data\n",
    "# !kaggle competitions download -c digit-recognizer\n",
    "# !unzip digit-recognizer.zip"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch import Generator\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n0      1       0       0       0       0       0       0       0       0   \n1      0       0       0       0       0       0       0       0       0   \n2      1       0       0       0       0       0       0       0       0   \n3      4       0       0       0       0       0       0       0       0   \n4      0       0       0       0       0       0       0       0       0   \n\n   pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n0       0  ...         0         0         0         0         0         0   \n1       0  ...         0         0         0         0         0         0   \n2       0  ...         0         0         0         0         0         0   \n3       0  ...         0         0         0         0         0         0   \n4       0  ...         0         0         0         0         0         0   \n\n   pixel780  pixel781  pixel782  pixel783  \n0         0         0         0         0  \n1         0         0         0         0  \n2         0         0         0         0  \n3         0         0         0         0  \n4         0         0         0         0  \n\n[5 rows x 785 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>pixel0</th>\n      <th>pixel1</th>\n      <th>pixel2</th>\n      <th>pixel3</th>\n      <th>pixel4</th>\n      <th>pixel5</th>\n      <th>pixel6</th>\n      <th>pixel7</th>\n      <th>pixel8</th>\n      <th>...</th>\n      <th>pixel774</th>\n      <th>pixel775</th>\n      <th>pixel776</th>\n      <th>pixel777</th>\n      <th>pixel778</th>\n      <th>pixel779</th>\n      <th>pixel780</th>\n      <th>pixel781</th>\n      <th>pixel782</th>\n      <th>pixel783</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 785 columns</p>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training samples is: 42000\n"
     ]
    }
   ],
   "source": [
    "print('number of training samples is:', len(train))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "(42000, 1, 28, 28)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x = i * 28 + j, where i is the row and j is the column\n",
    "# transfer the data to a 28x28 tensor matrix\n",
    "train_x = train.iloc[:, 1:].values.astype(float).reshape(-1, 1, 28, 28)\n",
    "train_y = train.iloc[:, 0].values\n",
    "train_x.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "255"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.max().max()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Create image dataset for preprocessing and augmentation\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, ids, X_transform=None, y_transform=None):\n",
    "        self.ids=ids\n",
    "        self.X_transform=X_transform\n",
    "        self.y_transform=y_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = self.ids.iloc[index, 1:].values.astype('float32').reshape(1, 28, 28)\n",
    "        if self.X_transform:\n",
    "            image = self.X_transform(image)\n",
    "\n",
    "        label = self.ids.iloc[index, 0]\n",
    "        if self.y_transform:\n",
    "            label = self.y_transform(label)\n",
    "\n",
    "        return image, label"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def imageDataLoader(\n",
    "        raw_data,\n",
    "        batch_size=64,\n",
    "        train_test_split=0.3,\n",
    "        seed=0,\n",
    "        X_transform=None,\n",
    "        y_transform=None\n",
    "):\n",
    "\n",
    "    dataset = ImageDataset(raw_data, X_transform=X_transform, y_transform=y_transform)\n",
    "    train_set, val_set = random_split(\n",
    "        dataset,\n",
    "        [(1-train_test_split), train_test_split],\n",
    "        generator=Generator().manual_seed(seed)\n",
    "    )\n",
    "\n",
    "    #DataLoaders\n",
    "    train_loader = DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True)\n",
    "    val_loader= DataLoader(dataset=val_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return train_loader, val_loader"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "X_transform = transforms.Compose([\n",
    "    # transforms.ToTensor(),\n",
    "    transforms.RandomRotation((-10.0, 10.0)),\n",
    "    transforms.RandomSolarize(p=0.5, threshold=0.5),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1,0.1)),\n",
    "])\n",
    "\n",
    "y_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "train_loader, val_loader = imageDataLoader(\n",
    "    train,\n",
    "    batch_size=64,\n",
    "    train_test_split=0.3,\n",
    "    seed=0,\n",
    "    # X_transform=X_transform,\n",
    "    # y_transform=y_transform\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "torch.Size([64])\n",
      "torch.int64\n"
     ]
    }
   ],
   "source": [
    "for X, y in train_loader:\n",
    "    print(X.dtype)\n",
    "    print(y.shape)\n",
    "    print(y.dtype)\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([64, 1, 28, 28])\n",
      "Labels batch shape: torch.Size([64])\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 0\n"
     ]
    }
   ],
   "source": [
    "train_features, train_labels = next(iter(train_loader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "img = train_features[0].squeeze()\n",
    "label = train_labels[0]\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()\n",
    "print(f\"Label: {label}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Build the model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# Define the model: MLP\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "        self.fc1 = nn.Linear(1600, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(F.max_pool2d(x, 2))\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(F.max_pool2d(x, 2))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train the model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ExponentialLR"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "!mkdir -p ~/DataspellProjects/kaggle/digit-recognizer/models\n",
    "!mkdir -p ~/DataspellProjects/kaggle/digit-recognizer/tb_logs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# default `log_dir` is \"runs\" - we'll be more specific here\n",
    "writer = SummaryWriter('tb_logs/MLP')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "model = CNN()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "scheduler = ExponentialLR(optimizer, gamma=0.9)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/10 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f913e835191744299fceec13275f0734"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xiaoziqi/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/functional.py:1331: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1: Training Loss: 0.7288 \tValidation Loss: 0.1331 \tTraining Accuracy: 0.8084 \tValidation Accuracy: 0.9620\n",
      "Epoch: 2: Training Loss: 0.2143 \tValidation Loss: 0.1030 \tTraining Accuracy: 0.9393 \tValidation Accuracy: 0.9713\n",
      "Epoch: 3: Training Loss: 0.1565 \tValidation Loss: 0.0784 \tTraining Accuracy: 0.9551 \tValidation Accuracy: 0.9797\n",
      "Epoch: 4: Training Loss: 0.1282 \tValidation Loss: 0.0792 \tTraining Accuracy: 0.9622 \tValidation Accuracy: 0.9790\n",
      "Epoch: 5: Training Loss: 0.1080 \tValidation Loss: 0.0657 \tTraining Accuracy: 0.9677 \tValidation Accuracy: 0.9813\n",
      "Epoch: 6: Training Loss: 0.1005 \tValidation Loss: 0.0664 \tTraining Accuracy: 0.9706 \tValidation Accuracy: 0.9817\n",
      "Epoch: 7: Training Loss: 0.0875 \tValidation Loss: 0.0709 \tTraining Accuracy: 0.9736 \tValidation Accuracy: 0.9825\n",
      "Epoch: 8: Training Loss: 0.0766 \tValidation Loss: 0.0643 \tTraining Accuracy: 0.9770 \tValidation Accuracy: 0.9837\n",
      "Epoch: 9: Training Loss: 0.0663 \tValidation Loss: 0.0637 \tTraining Accuracy: 0.9796 \tValidation Accuracy: 0.9849\n",
      "Epoch: 10: Training Loss: 0.0667 \tValidation Loss: 0.0617 \tTraining Accuracy: 0.9803 \tValidation Accuracy: 0.9848\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.has_mps else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "epochs = 10\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "    best_val_acc = 0\n",
    "\n",
    "    model.to(device) # set the model to the device\n",
    "    model.train() # set the model to training mode\n",
    "    # training loop\n",
    "    for X, y in train_loader:\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X)\n",
    "\n",
    "        loss = criterion(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * X.size(0)\n",
    "        train_acc += (output.argmax(1) == y).cpu().type(torch.float).sum()\n",
    "\n",
    "    writer.add_scalar('training loss',\n",
    "                      train_loss / len(train_loader.dataset),\n",
    "                      epoch+1)\n",
    "\n",
    "    writer.add_scalar('training accuracy',\n",
    "                      train_acc / len(train_loader.dataset),\n",
    "                      epoch+1)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad(): # set the model to evaluation mode\n",
    "        # validation loop\n",
    "        for X, y in val_loader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            output = model(X)\n",
    "            loss = criterion(output, y)\n",
    "            val_loss += loss.item() * X.size(0)\n",
    "            val_acc += (output.argmax(1) == y).cpu().type(torch.float).sum()\n",
    "\n",
    "    writer.add_scalar('validation loss',\n",
    "                      val_loss / len(val_loader.dataset),\n",
    "                      epoch+1)\n",
    "\n",
    "    writer.add_scalar('validation accuracy',\n",
    "                      val_acc / len(val_loader.dataset),\n",
    "                      epoch+1)\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), 'models/cnn_model.pt')\n",
    "\n",
    "    train_loss = train_loss / len(train_loader.dataset)\n",
    "    train_acc = train_acc / len(train_loader.dataset)\n",
    "    val_loss = val_loss / len(val_loader.dataset)\n",
    "    val_acc = val_acc / len(val_loader.dataset)\n",
    "    print(f'Epoch: {epoch + 1}: Training Loss: {train_loss:.4f} \\tValidation Loss: {val_loss:.4f} \\tTraining Accuracy: {train_acc:.4f} \\tValidation Accuracy: {val_acc:.4f}')\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "writer.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "# %load_ext tensorboard"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "# %tensorboard --logdir=tb_logs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test the model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "test = pd.read_csv('data/test.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "test = test.values.reshape(-1, 28, 28)\n",
    "test = torch.from_numpy(test)\n",
    "test = test.unsqueeze(1)\n",
    "test = test.type(torch.float)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "CNN(\n  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n  (dropout1): Dropout2d(p=0.25, inplace=False)\n  (dropout2): Dropout2d(p=0.5, inplace=False)\n  (fc1): Linear(in_features=1600, out_features=128, bias=True)\n  (fc2): Linear(in_features=128, out_features=10, bias=True)\n)"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CNN()\n",
    "model.load_state_dict(torch.load('models/cnn_model.pt'))\n",
    "model.eval()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "CNN(\n  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n  (dropout1): Dropout2d(p=0.25, inplace=False)\n  (dropout2): Dropout2d(p=0.5, inplace=False)\n  (fc1): Linear(in_features=1600, out_features=128, bias=True)\n  (fc2): Linear(in_features=128, out_features=10, bias=True)\n)"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.has_mps else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "model.to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xiaoziqi/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/functional.py:1331: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "test = test.to(device)\n",
    "output = model(test)\n",
    "output = output.argmax(1).cpu().numpy()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "array([2, 0, 9, ..., 3, 9, 2])"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "!mkdir -p ~/DataspellProjects/kaggle/digit-recognizer/submissions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'ImageId': range(1, len(output)+1), 'Label': output})\n",
    "submission.to_csv('submissions/cnn_submission.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /Users/xiaoziqi/.kaggle/kaggle.json'\r\n",
      "100%|████████████████████████████████████████| 208k/208k [00:03<00:00, 55.0kB/s]\r\n",
      "Successfully submitted to Digit Recognizer"
     ]
    }
   ],
   "source": [
    "# make submission\n",
    "!kaggle competitions submit -c digit-recognizer -f ~/DataspellProjects/kaggle/digit-recognizer/submissions/cnn_submission.csv -m \"CNN submission\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
