{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the directory structure and download the data from Kaggle\n",
    "# !mkdir -p ~/DataspellProjects/kaggle/digit-recognizer/data\n",
    "# !cd ~/DataspellProjects/kaggle/digit-recognizer/data\n",
    "# !kaggle competitions download -c digit-recognizer\n",
    "# !unzip digit-recognizer.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch import Generator\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
       "0      1       0       0       0       0       0       0       0       0   \n",
       "1      0       0       0       0       0       0       0       0       0   \n",
       "2      1       0       0       0       0       0       0       0       0   \n",
       "3      4       0       0       0       0       0       0       0       0   \n",
       "4      0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "0       0  ...         0         0         0         0         0         0   \n",
       "1       0  ...         0         0         0         0         0         0   \n",
       "2       0  ...         0         0         0         0         0         0   \n",
       "3       0  ...         0         0         0         0         0         0   \n",
       "4       0  ...         0         0         0         0         0         0   \n",
       "\n",
       "   pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0  \n",
       "1         0         0         0         0  \n",
       "2         0         0         0         0  \n",
       "3         0         0         0         0  \n",
       "4         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training samples is: 42000\n"
     ]
    }
   ],
   "source": [
    "print('number of training samples is:', len(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42000, 1, 28, 28)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x = i * 28 + j, where i is the row and j is the column\n",
    "# transfer the data to a 28x28 tensor matrix\n",
    "train_x = train.iloc[:, 1:].values.astype(float).reshape(-1, 1, 28, 28)\n",
    "train_y = train.iloc[:, 0].values\n",
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.max().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create image dataset for preprocessing and augmentation\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, ids, X_transform=None, y_transform=None):\n",
    "        self.ids=ids\n",
    "        self.X_transform=X_transform\n",
    "        self.y_transform=y_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = self.ids.iloc[index, 1:].values.astype('float32').reshape(1, 28, 28)\n",
    "        if self.X_transform:\n",
    "            image = self.X_transform(image)\n",
    "\n",
    "        label = self.ids.iloc[index, 0]\n",
    "        if self.y_transform:\n",
    "            label = self.y_transform(label)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def imageDataLoader(\n",
    "        raw_data,\n",
    "        batch_size=64,\n",
    "        train_test_split=0.3,\n",
    "        seed=0,\n",
    "        X_transform=None,\n",
    "        y_transform=None\n",
    "):\n",
    "\n",
    "    dataset = ImageDataset(raw_data, X_transform=X_transform, y_transform=y_transform)\n",
    "    train_set, val_set = random_split(\n",
    "        dataset,\n",
    "        [(1-train_test_split), train_test_split],\n",
    "        generator=Generator().manual_seed(seed)\n",
    "    )\n",
    "\n",
    "    #DataLoaders\n",
    "    train_loader = DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True)\n",
    "    val_loader= DataLoader(dataset=val_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "y_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_loader, val_loader = imageDataLoader(\n",
    "    train,\n",
    "    batch_size=64,\n",
    "    train_test_split=0.3,\n",
    "    seed=0,\n",
    "    # X_transform=X_transform,\n",
    "    # y_transform=y_transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "torch.Size([64])\n",
      "torch.int64\n"
     ]
    }
   ],
   "source": [
    "for X, y in train_loader:\n",
    "    print(X.dtype)\n",
    "    print(y.shape)\n",
    "    print(y.dtype)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([64, 1, 28, 28])\n",
      "Labels batch shape: torch.Size([64])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 8\n"
     ]
    }
   ],
   "source": [
    "train_features, train_labels = next(iter(train_loader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "img = train_features[0].squeeze()\n",
    "label = train_labels[0]\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define the model: MLP\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!mkdir -p ~/DataspellProjects/kaggle/digit-recognizer/models\n",
    "!mkdir -p ~/DataspellProjects/kaggle/digit-recognizer/tb_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# default `log_dir` is \"runs\" - we'll be more specific here\n",
    "writer = SummaryWriter('tb_logs/MLP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = MLP()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65cf2afc7f274104b7201a904e20a7e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1: Training Loss: 0.4426 \tValidation Loss: 0.2016 \tTraining Accuracy: 0.9079 \tValidation Accuracy: 0.9402\n",
      "Epoch: 2: Training Loss: 0.1394 \tValidation Loss: 0.1982 \tTraining Accuracy: 0.9577 \tValidation Accuracy: 0.9475\n",
      "Epoch: 3: Training Loss: 0.1076 \tValidation Loss: 0.2210 \tTraining Accuracy: 0.9664 \tValidation Accuracy: 0.9483\n",
      "Epoch: 4: Training Loss: 0.0997 \tValidation Loss: 0.1657 \tTraining Accuracy: 0.9709 \tValidation Accuracy: 0.9570\n",
      "Epoch: 5: Training Loss: 0.0880 \tValidation Loss: 0.1793 \tTraining Accuracy: 0.9733 \tValidation Accuracy: 0.9610\n",
      "Epoch: 6: Training Loss: 0.0799 \tValidation Loss: 0.1835 \tTraining Accuracy: 0.9760 \tValidation Accuracy: 0.9630\n",
      "Epoch: 7: Training Loss: 0.0825 \tValidation Loss: 0.2035 \tTraining Accuracy: 0.9765 \tValidation Accuracy: 0.9587\n",
      "Epoch: 8: Training Loss: 0.0746 \tValidation Loss: 0.2065 \tTraining Accuracy: 0.9795 \tValidation Accuracy: 0.9623\n",
      "Epoch: 9: Training Loss: 0.0713 \tValidation Loss: 0.1974 \tTraining Accuracy: 0.9796 \tValidation Accuracy: 0.9611\n",
      "Epoch: 10: Training Loss: 0.0668 \tValidation Loss: 0.2060 \tTraining Accuracy: 0.9822 \tValidation Accuracy: 0.9621\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.has_mps else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "epochs = 10\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "    best_val_acc = 0\n",
    "\n",
    "    model.to(device) # set the model to the device\n",
    "    model.train() # set the model to training mode\n",
    "    # training loop\n",
    "    for X, y in train_loader:\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X)\n",
    "\n",
    "        loss = criterion(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * X.size(0)\n",
    "        train_acc += (output.argmax(1) == y).cpu().type(torch.float).sum()\n",
    "\n",
    "    writer.add_scalar('training loss',\n",
    "                      train_loss / len(train_loader.dataset),\n",
    "                      epoch+1)\n",
    "\n",
    "    writer.add_scalar('training accuracy',\n",
    "                      train_acc / len(train_loader.dataset),\n",
    "                      epoch+1)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad(): # set the model to evaluation mode\n",
    "        # validation loop\n",
    "        for X, y in val_loader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            output = model(X)\n",
    "            loss = criterion(output, y)\n",
    "            val_loss += loss.item() * X.size(0)\n",
    "            val_acc += (output.argmax(1) == y).cpu().type(torch.float).sum()\n",
    "\n",
    "    writer.add_scalar('validation loss',\n",
    "                      val_loss / len(val_loader.dataset),\n",
    "                      epoch+1)\n",
    "\n",
    "    writer.add_scalar('validation accuracy',\n",
    "                      val_acc / len(val_loader.dataset),\n",
    "                      epoch+1)\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), 'models/model.pt')\n",
    "\n",
    "    train_loss = train_loss / len(train_loader.dataset)\n",
    "    train_acc = train_acc / len(train_loader.dataset)\n",
    "    val_loss = val_loss / len(val_loader.dataset)\n",
    "    val_acc = val_acc / len(val_loader.dataset)\n",
    "    print(f'Epoch: {epoch + 1}: Training Loss: {train_loss:.4f} \\tValidation Loss: {val_loss:.4f} \\tTraining Accuracy: {train_acc:.4f} \\tValidation Accuracy: {val_acc:.4f}')\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Launching TensorBoard..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-2922e2d0b5cf5f90\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-2922e2d0b5cf5f90\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir=tb_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = test.values.reshape(-1, 28, 28)\n",
    "test = torch.from_numpy(test)\n",
    "test = test.unsqueeze(1)\n",
    "test = test.type(torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (fc1): Linear(in_features=784, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (fc3): Linear(in_features=256, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MLP()\n",
    "model.load_state_dict(torch.load('models/model.pt'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (fc1): Linear(in_features=784, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (fc3): Linear(in_features=256, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.has_mps else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = test.to(device)\n",
    "output = model(test)\n",
    "output = output.argmax(1).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 9, ..., 3, 9, 2])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!mkdir -p ~/DataspellProjects/kaggle/digit-recognizer/submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'ImageId': range(1, len(output)+1), 'Label': output})\n",
    "submission.to_csv('submissions/mlp_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /Users/xiaoziqi/.kaggle/kaggle.json'\r\n",
      "100%|████████████████████████████████████████| 208k/208k [00:03<00:00, 65.2kB/s]\r\n",
      "Successfully submitted to Digit Recognizer"
     ]
    }
   ],
   "source": [
    "# make submission\n",
    "!kaggle competitions submit -c digit-recognizer -f ~/DataspellProjects/kaggle/digit-recognizer/submissions/mlp_submission.csv -m \"MLP submission\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
